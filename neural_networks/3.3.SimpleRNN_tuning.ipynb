{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.metrics import mean_absolute_error\nimport tensorflow as tf\nimport optuna\nimport os\nimport random\nimport datetime\n\nnp.random.seed(42)\ntf.random.set_seed(42)\nrandom.seed(42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:05:08.679202Z","iopub.execute_input":"2025-01-02T22:05:08.679609Z","iopub.status.idle":"2025-01-02T22:05:19.370946Z","shell.execute_reply.started":"2025-01-02T22:05:08.679576Z","shell.execute_reply":"2025-01-02T22:05:19.369821Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"tf.config.list_physical_devices()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:05:19.372198Z","iopub.execute_input":"2025-01-02T22:05:19.372710Z","iopub.status.idle":"2025-01-02T22:05:19.382429Z","shell.execute_reply.started":"2025-01-02T22:05:19.372684Z","shell.execute_reply":"2025-01-02T22:05:19.381266Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"data_path = 'https://raw.githubusercontent.com/antbartash/max_temp/master/data/data2.csv'\ndata = pd.read_csv(data_path, index_col=0)\ndata['DATE'] = data['DATE'].astype('datetime64[ns]')\n\nprint(data.shape)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:05:19.384286Z","iopub.execute_input":"2025-01-02T22:05:19.384546Z","iopub.status.idle":"2025-01-02T22:05:19.831110Z","shell.execute_reply.started":"2025-01-02T22:05:19.384523Z","shell.execute_reply":"2025-01-02T22:05:19.830064Z"}},"outputs":[{"name":"stdout","text":"(40898, 4)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"       STATION                        NAME       DATE  TMAX\n0  USW00012916  NEW ORLEANS AIRPORT, LA US 2010-01-01  12.2\n1  USW00012916  NEW ORLEANS AIRPORT, LA US 2010-01-02  10.6\n2  USW00012916  NEW ORLEANS AIRPORT, LA US 2010-01-03   8.3\n3  USW00012916  NEW ORLEANS AIRPORT, LA US 2010-01-04   6.1\n4  USW00012916  NEW ORLEANS AIRPORT, LA US 2010-01-05   6.1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>STATION</th>\n      <th>NAME</th>\n      <th>DATE</th>\n      <th>TMAX</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>USW00012916</td>\n      <td>NEW ORLEANS AIRPORT, LA US</td>\n      <td>2010-01-01</td>\n      <td>12.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>USW00012916</td>\n      <td>NEW ORLEANS AIRPORT, LA US</td>\n      <td>2010-01-02</td>\n      <td>10.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>USW00012916</td>\n      <td>NEW ORLEANS AIRPORT, LA US</td>\n      <td>2010-01-03</td>\n      <td>8.3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>USW00012916</td>\n      <td>NEW ORLEANS AIRPORT, LA US</td>\n      <td>2010-01-04</td>\n      <td>6.1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>USW00012916</td>\n      <td>NEW ORLEANS AIRPORT, LA US</td>\n      <td>2010-01-05</td>\n      <td>6.1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"data = data[['STATION', 'DATE', 'TMAX']]\n\n# add sine and cosine transforms to add periodicality\ndoy = data['DATE'].dt.dayofyear / 365.25\ndata['Year_sin'] = np.sin(doy * 2 * np.pi)\ndata['Year_cos'] = np.cos(doy * 2 * np.pi)\n\nprint(data.shape)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:05:19.832607Z","iopub.execute_input":"2025-01-02T22:05:19.832900Z","iopub.status.idle":"2025-01-02T22:05:19.857750Z","shell.execute_reply.started":"2025-01-02T22:05:19.832875Z","shell.execute_reply":"2025-01-02T22:05:19.856724Z"}},"outputs":[{"name":"stdout","text":"(40898, 5)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"       STATION       DATE  TMAX  Year_sin  Year_cos\n0  USW00012916 2010-01-01  12.2  0.017202  0.999852\n1  USW00012916 2010-01-02  10.6  0.034398  0.999408\n2  USW00012916 2010-01-03   8.3  0.051584  0.998669\n3  USW00012916 2010-01-04   6.1  0.068755  0.997634\n4  USW00012916 2010-01-05   6.1  0.085906  0.996303","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>STATION</th>\n      <th>DATE</th>\n      <th>TMAX</th>\n      <th>Year_sin</th>\n      <th>Year_cos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>USW00012916</td>\n      <td>2010-01-01</td>\n      <td>12.2</td>\n      <td>0.017202</td>\n      <td>0.999852</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>USW00012916</td>\n      <td>2010-01-02</td>\n      <td>10.6</td>\n      <td>0.034398</td>\n      <td>0.999408</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>USW00012916</td>\n      <td>2010-01-03</td>\n      <td>8.3</td>\n      <td>0.051584</td>\n      <td>0.998669</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>USW00012916</td>\n      <td>2010-01-04</td>\n      <td>6.1</td>\n      <td>0.068755</td>\n      <td>0.997634</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>USW00012916</td>\n      <td>2010-01-05</td>\n      <td>6.1</td>\n      <td>0.085906</td>\n      <td>0.996303</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{"execution":{"iopub.status.busy":"2024-12-28T11:24:06.993325Z","iopub.execute_input":"2024-12-28T11:24:06.993684Z","iopub.status.idle":"2024-12-28T11:24:06.997628Z","shell.execute_reply.started":"2024-12-28T11:24:06.993646Z","shell.execute_reply":"2024-12-28T11:24:06.996626Z"}}},{"cell_type":"code","source":"# TRAIN/VALID/TEST SPLIT\n\ntrain_df = data.loc[data['DATE'].dt.year <= 2021].reset_index(drop=True).copy()\nvalid_df = data.loc[data['DATE'].dt.year == 2022].reset_index(drop=True).copy()\ntest_df = data.loc[data['DATE'].dt.year == 2023].reset_index(drop=True).copy()\n\nprint(f'Train: {train_df.shape}')\nprint(f'Valid: {valid_df.shape}')\nprint(f'Test: {test_df.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:05:19.858842Z","iopub.execute_input":"2025-01-02T22:05:19.859160Z","iopub.status.idle":"2025-01-02T22:05:19.883619Z","shell.execute_reply.started":"2025-01-02T22:05:19.859133Z","shell.execute_reply":"2025-01-02T22:05:19.882658Z"}},"outputs":[{"name":"stdout","text":"Train: (35058, 5)\nValid: (2920, 5)\nTest: (2920, 5)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# SCALING\n\ntrain_df.drop(columns=['DATE'], inplace=True)\nvalid_df.drop(columns=['DATE'], inplace=True)\ntest_df.drop(columns=['DATE'], inplace=True)\n\n# keep station to drop mixed windows later (encode to avoid errors from scaler)\nencoder = OrdinalEncoder()\nencoder.fit(train_df[['STATION']])\ntrain_df['STATION'] = encoder.transform(train_df[['STATION']])[:, 0]\nvalid_df['STATION'] = encoder.transform(valid_df[['STATION']])[:, 0]\ntest_df['STATION'] = encoder.transform(test_df[['STATION']])[:, 0]\n\n# scaling\nscaler = StandardScaler()\nscaler.fit(train_df)\ntrain_df = pd.DataFrame(scaler.transform(train_df),\n                        columns=scaler.feature_names_in_, index=train_df.index)\nvalid_df = pd.DataFrame(scaler.transform(valid_df),\n                        columns=scaler.feature_names_in_, index=valid_df.index)\ntest_df = pd.DataFrame(scaler.transform(test_df),\n                       columns=scaler.feature_names_in_, index=test_df.index)\n\ntrain_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:05:19.884867Z","iopub.execute_input":"2025-01-02T22:05:19.885174Z","iopub.status.idle":"2025-01-02T22:05:19.925698Z","shell.execute_reply.started":"2025-01-02T22:05:19.885149Z","shell.execute_reply":"2025-01-02T22:05:19.924638Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"    STATION      TMAX  Year_sin  Year_cos\n0 -1.091014 -0.747812  0.024401  1.414059\n1 -1.091014 -0.896450  0.048721  1.413431\n2 -1.091014 -1.110117  0.073027  1.412386\n3 -1.091014 -1.314495  0.097312  1.410922\n4 -1.091014 -1.314495  0.121567  1.409041","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>STATION</th>\n      <th>TMAX</th>\n      <th>Year_sin</th>\n      <th>Year_cos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.091014</td>\n      <td>-0.747812</td>\n      <td>0.024401</td>\n      <td>1.414059</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.091014</td>\n      <td>-0.896450</td>\n      <td>0.048721</td>\n      <td>1.413431</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.091014</td>\n      <td>-1.110117</td>\n      <td>0.073027</td>\n      <td>1.412386</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.091014</td>\n      <td>-1.314495</td>\n      <td>0.097312</td>\n      <td>1.410922</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.091014</td>\n      <td>-1.314495</td>\n      <td>0.121567</td>\n      <td>1.409041</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def tmax_inverse_transform(arr, scale=scaler.scale_[1], mean=scaler.mean_[1]):\n    return arr * scale + mean","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:05:19.926629Z","iopub.execute_input":"2025-01-02T22:05:19.926889Z","iopub.status.idle":"2025-01-02T22:05:19.931594Z","shell.execute_reply.started":"2025-01-02T22:05:19.926867Z","shell.execute_reply":"2025-01-02T22:05:19.930443Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# TF DATASET\n\ndef create_dataset(data, target_col, source_col, seq_length):\n  input_data = data[:-seq_length]\n  # adding source_col to target allows to drop samples with features and target\n  # from diff sources (source_col will be dropped from the target later)\n  targets = data[[source_col, target_col]][seq_length:]\n  dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n      input_data, targets,\n      sequence_length=seq_length,\n      sequence_stride=1,\n      batch_size=32,\n      shuffle=False,\n      seed=42\n    )\n  for batch in dataset:\n    inputs, targets = batch\n    assert np.array_equal(inputs[0], data[:seq_length])  # First sequence: steps [0-13]\n    # Corresponding target: step 14\n    assert np.array_equal(targets[0, 1], data.loc[seq_length, target_col])\n    break\n  return dataset\n\ntrain_ds = create_dataset(train_df, 'TMAX', 'STATION', 14)\nvalid_ds = create_dataset(valid_df, 'TMAX', 'STATION', 14)\ntest_ds = create_dataset(test_df, 'TMAX', 'STATION', 14)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:05:19.933853Z","iopub.execute_input":"2025-01-02T22:05:19.934157Z","iopub.status.idle":"2025-01-02T22:05:20.455912Z","shell.execute_reply.started":"2025-01-02T22:05:19.934133Z","shell.execute_reply":"2025-01-02T22:05:20.455082Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# DATASET CLEANING\n\ndef filter_mixed_windows(dataset):\n    def is_valid_window(inputs, targets):\n        # Extract the first feature (station) from inputs and targets\n        input_station_ids = inputs[:, :, 0]  # Shape: (batch_size, sequence_length)\n        target_station_ids = targets[:, 0]  # Shape: (batch_size)\n\n        # Check if all station IDs in the inputs are the same\n        input_same_station = tf.reduce_all(tf.reduce_max(input_station_ids, axis=1) == tf.reduce_min(input_station_ids, axis=1))\n\n        # Check if the target's station ID matches the input station ID\n        target_matches_input = tf.reduce_all(tf.reduce_max(input_station_ids, axis=1) == target_station_ids)\n\n        # Only keep windows where both conditions are true\n        return tf.logical_and(input_same_station, target_matches_input)\n\n    # Filter the dataset\n    filtered_dataset = dataset.filter(is_valid_window)\n    return filtered_dataset\n\ntrain_ds = filter_mixed_windows(train_ds)\nvalid_ds = filter_mixed_windows(valid_ds)\ntest_ds = filter_mixed_windows(test_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:05:20.457775Z","iopub.execute_input":"2025-01-02T22:05:20.458201Z","iopub.status.idle":"2025-01-02T22:05:20.799051Z","shell.execute_reply.started":"2025-01-02T22:05:20.458161Z","shell.execute_reply":"2025-01-02T22:05:20.798032Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def drop_first_column(feature, label):\n    feature = feature[:, :, 1:]  # Keep all rows, drop the first column\n    label = label[:, 1:]    # Keep all rows, drop the first column\n    return feature, label\n\ntrain_ds = train_ds.map(drop_first_column)\nvalid_ds = valid_ds.map(drop_first_column)\ntest_ds = test_ds.map(drop_first_column)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:05:20.799996Z","iopub.execute_input":"2025-01-02T22:05:20.800287Z","iopub.status.idle":"2025-01-02T22:05:20.882440Z","shell.execute_reply.started":"2025-01-02T22:05:20.800262Z","shell.execute_reply":"2025-01-02T22:05:20.881428Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def load_data(batch_size, prefetch=tf.data.AUTOTUNE,\n              train_ds=train_ds, valid_ds=valid_ds, test_ds=test_ds):\n    train_ds = train_ds.rebatch(batch_size).prefetch(prefetch)\n    valid_ds = valid_ds.rebatch(batch_size).prefetch(prefetch)\n    test_ds = test_ds.rebatch(batch_size).prefetch(prefetch)\n    train_num_batches = len(list(train_ds))\n    valid_num_batches = len(list(valid_ds))\n    test_num_batches = len(list(test_ds))\n    # print(f'load_data - num_batches (train, valid, test): {train_num_batches}, {valid_num_batches}, {test_num_batches}')\n    train_ds_repeat = train_ds.repeat()\n    valid_ds_repeat = valid_ds.repeat()\n    test_ds_repeat = test_ds.repeat()\n    return train_ds_repeat, valid_ds_repeat, test_ds_repeat, train_num_batches, valid_num_batches, test_num_batches\n\ntrain_ds_repeat, valid_ds_repeat, test_ds_repeat, train_num_batches, valid_num_batches, test_num_batches = load_data(batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:05:20.883600Z","iopub.execute_input":"2025-01-02T22:05:20.883904Z","iopub.status.idle":"2025-01-02T22:05:24.465347Z","shell.execute_reply.started":"2025-01-02T22:05:20.883878Z","shell.execute_reply":"2025-01-02T22:05:24.464308Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# NOTE: THE FUNCTION DOES NOT INCLUDE BIRNN OPTION DUE TO INTERPRETABILITY ISSUE WITH THE WEATHER DATASET\n\ndef create_model(trial, input_shape):\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Input(shape=input_shape))\n    \n    # RNN LAYERS\n    rnn_num_layers = trial.suggest_int('rnn_num_layers', 1, 4)\n    units = trial.suggest_categorical(f'units_rnn', [2, 4, 8, 16, 32])\n    for rnn_layer_num in range(rnn_num_layers):\n        actv_func = trial.suggest_categorical(f'actv_func_rnn_{rnn_layer_num}', ['relu', 'leaky_relu', 'elu', 'sigmoid', 'tanh', 'gelu'])\n        \n        bias_reg_init = trial.suggest_categorical(f'bias_reg_init_rnn_layer{rnn_layer_num}', ['l1', 'l2', 'l1l2', None])\n        if bias_reg_init == 'l1':\n            bias_reg = tf.keras.regularizers.l1(trial.suggest_float(f'bias_reg_rnn_layer_{rnn_layer_num}', 0.0001, 0.5))\n        elif bias_reg_init == 'l2':\n            bias_reg = tf.keras.regularizers.l2(trial.suggest_float(f'bias_reg_rnn_layer_{rnn_layer_num}', 0.0001, 0.5))\n        elif bias_reg_init == 'l1l2':\n            bias_reg = tf.keras.regularizers.L1L2(trial.suggest_float(f'bias_reg_rnn_layer_{rnn_layer_num}_l1', 0.0001, 0.5),\n                                                  trial.suggest_float(f'bias_reg_rnn_layer_{rnn_layer_num}_l2', 0.0001, 0.5))\n        else:\n            bias_reg = None\n\n        kernel_reg_init = trial.suggest_categorical(f'kernel_reg_init_rnn_layer_{rnn_layer_num}', ['l1', 'l2', 'l1l2', None])\n        if kernel_reg_init == 'l1':\n            kernel_reg = tf.keras.regularizers.l1(trial.suggest_float(f'kernel_reg_rnn_layer_{rnn_layer_num}', 0.0001, 0.5))\n        elif kernel_reg_init == 'l2':\n            kernel_reg = tf.keras.regularizers.l2(trial.suggest_float(f'kernel_reg_rnn_layer_{rnn_layer_num}', 0.0001, 0.5))\n        elif kernel_reg_init == 'l1l2':\n            kernel_reg = tf.keras.regularizers.L1L2(trial.suggest_float(f'kernel_reg_rnn_layer_{rnn_layer_num}_l1', 0.0001, 0.5),\n                                                    trial.suggest_float(f'kernel_reg_rnn_layer_{rnn_layer_num}_l2', 0.0001, 0.5))\n        else:\n            kernel_reg = None\n\n        kernel_initializer = trial.suggest_categorical(\n            f'kernel_initializer_rnn_layer_{rnn_layer_num}',\n            ['glorot_uniform', 'glorot_normal', 'he_uniform', 'he_normal', 'lecun_uniform', 'lecun_normal']\n        )\n\n        if rnn_layer_num == rnn_num_layers-1: # last rnn layer \n            return_sequences = False \n        else: \n            return_sequences = True\n\n        \n        model.add(\n            tf.keras.layers.SimpleRNN(\n                units=units,\n                activation=actv_func,\n                kernel_initializer=kernel_initializer,\n                bias_regularizer=bias_reg,\n                kernel_regularizer=kernel_reg,\n                return_sequences=return_sequences\n            )\n        )\n\n    \n    # FLATTEN\n    model.add(tf.keras.layers.Flatten())\n    \n    # DENSE LAYERS\n    # layer 0\n    n_units_0 = trial.suggest_int('nunints_layer_0', 32, 512, step=32)\n    actv_func_0 = trial.suggest_categorical('actv_func_layer_0', ['relu', 'leaky_relu', 'elu', 'sigmoid', 'tanh', 'gelu'])\n\n    bias_reg_init_0 = trial.suggest_categorical('bias_reg_init_layer_0', ['l1', 'l2', 'l1l2', None])\n    if bias_reg_init_0 == 'l1':\n        bias_reg_0 = tf.keras.regularizers.l1(trial.suggest_float('bias_reg_layer_0', 0.0001, 0.5))\n    elif bias_reg_init_0 == 'l2':\n        bias_reg_0 = tf.keras.regularizers.l2(trial.suggest_float('bias_reg_layer_0', 0.0001, 0.5))\n    elif bias_reg_init_0 == 'l1l2':\n        bias_reg_0 = tf.keras.regularizers.L1L2(trial.suggest_float('bias_reg_layer_0_l1', 0.0001, 0.5),\n                                                trial.suggest_float('bias_reg_layer_0_l2', 0.0001, 0.5))\n    else:\n        bias_reg_0 = None\n\n    kernel_reg_init_0 = trial.suggest_categorical('kernel_reg_init_layer_0', ['l1', 'l2', 'l1l2', None])\n    if kernel_reg_init_0 == 'l1':\n        kernel_reg_0 = tf.keras.regularizers.l1(trial.suggest_float('kernel_reg_layer_0', 0.0001, 0.5))\n    elif kernel_reg_init_0 == 'l2':\n        kernel_reg_0 = tf.keras.regularizers.l2(trial.suggest_float('kernel_reg_layer_0', 0.0001, 0.5))\n    elif kernel_reg_init_0 == 'l1l2':\n        kernel_reg_0 = tf.keras.regularizers.L1L2(trial.suggest_float('kernel_reg_layer_0_l1', 0.0001, 0.5),\n                                                  trial.suggest_float('kernel_reg_layer_0_l2', 0.0001, 0.5))\n    else:\n        kernel_reg_0 = None\n\n    kernel_initializer_0 = trial.suggest_categorical(\n        'kernel_initializer_layer_0', ['glorot_uniform', 'glorot_normal',\n                                       'he_uniform', 'he_normal',\n                                       'lecun_uniform', 'lecun_normal']\n    )\n    \n    model.add(\n        tf.keras.layers.Dense(\n            units=n_units_0, activation=actv_func_0, \n            kernel_initializer=kernel_initializer_0,\n            bias_regularizer=bias_reg_0,\n            kernel_regularizer=kernel_reg_0\n        )\n    )\n\n    \n    # hidden layers\n    num_layers = trial.suggest_int('num_layers', 0, 2)\n    batch_norm = trial.suggest_categorical(f'batch_norm', [True, False])\n    for layer_num in range(num_layers):\n        layer_i = layer_num + 1\n        n_units = trial.suggest_int(f'nunits_layer_{layer_i}', 32, 512, step=32)\n        actv_func = trial.suggest_categorical(f'actv_func_layer_{layer_i}', ['relu', 'leaky_relu', 'elu', 'sigmoid', 'tanh', 'gelu'])\n\n        bias_reg_init = trial.suggest_categorical(f'bias_reg_init_layer_{layer_i}', ['l1', 'l2', 'l1l2', None])\n        if bias_reg_init == 'l1':\n            bias_reg = tf.keras.regularizers.l1(trial.suggest_float(f'bias_reg_layer_{layer_i}', 0.0001, 0.5))\n        elif bias_reg_init == 'l2':\n            bias_reg = tf.keras.regularizers.l2(trial.suggest_float(f'bias_reg_layer_{layer_i}', 0.0001, 0.5))\n        elif bias_reg_init == 'l1l2':\n            bias_reg = tf.keras.regularizers.L1L2(trial.suggest_float(f'bias_reg_layer_{layer_i}_l1', 0.0001, 0.5),\n                                                  trial.suggest_float(f'bias_reg_layer_{layer_i}_l2', 0.0001, 0.5))\n        else:\n            bias_reg = None\n\n        kernel_reg_init = trial.suggest_categorical(f'kernel_reg_init_layer_{layer_i}', ['l1', 'l2', 'l1l2', None])\n        if kernel_reg_init == 'l1':\n            kernel_reg = tf.keras.regularizers.l1(trial.suggest_float(f'kernel_reg_layer_{layer_i}', 0.0001, 0.5))\n        elif kernel_reg_init == 'l2':\n            kernel_reg = tf.keras.regularizers.l2(trial.suggest_float(f'kernel_reg_layer_{layer_i}', 0.0001, 0.5))\n        elif kernel_reg_init == 'l1l2':\n            kernel_reg = tf.keras.regularizers.L1L2(trial.suggest_float(f'kernel_reg_layer_{layer_i}_l1', 0.0001, 0.5),\n                                                    trial.suggest_float(f'kernel_reg_layer_{layer_i}_l2', 0.0001, 0.5))\n        else:\n            kernel_reg = None\n\n        kernel_initializer = trial.suggest_categorical(\n            f'kernel_initializer_layer_{layer_i}', ['glorot_uniform', 'glorot_normal',\n                                                    'he_uniform', 'he_normal',\n                                                    'lecun_uniform', 'lecun_normal']\n        )\n        \n        if batch_norm:\n            model.add(tf.keras.layers.BatchNormalization())\n        dropout_rate = trial.suggest_float(f'dropout_rate_layer_{layer_i}', 0.0, 0.999)\n        model.add(tf.keras.layers.Dropout(dropout_rate))\n        model.add(\n            tf.keras.layers.Dense(\n                n_units, actv_func, \n                kernel_initializer=kernel_initializer,\n                bias_regularizer=bias_reg,\n                kernel_regularizer=kernel_reg\n            )\n        )\n    if batch_norm:\n        model.add(tf.keras.layers.BatchNormalization())\n    dropout_rate = trial.suggest_float(f'dropout_rate_layer_output', 0.0, 0.999)\n    model.add(tf.keras.layers.Dropout(dropout_rate))\n    model.add(tf.keras.layers.Dense(1, activation='linear'))\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:05:24.466466Z","iopub.execute_input":"2025-01-02T22:05:24.466895Z","iopub.status.idle":"2025-01-02T22:05:24.490586Z","shell.execute_reply.started":"2025-01-02T22:05:24.466850Z","shell.execute_reply":"2025-01-02T22:05:24.489421Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Optimizer","metadata":{}},{"cell_type":"code","source":"def create_optimizer(trial):\n    opt_kwargs = {}\n    opt_init = trial.suggest_categorical('optimizer', ['SGD', 'Adam', 'Nadam', 'Adamax'])\n    if opt_init == 'SGD':\n        opt_kwargs['learning_rate'] = trial.suggest_float('opt_lr', 1e-2, 1e-1, log=True)\n        opt_kwargs['momentum'] = trial.suggest_float('opt_momentum', 1e-5, 0.1, log=True)\n        opt_kwargs['nesterov'] = trial.suggest_categorical('opt_nesterov', [True, False])\n    if opt_init == 'Adam':\n        opt_kwargs['learning_rate'] = trial.suggest_float('opt_lr', 1e-2, 1e-1, log=True)\n        opt_kwargs['beta_1'] = trial.suggest_categorical('opt_beta_1', [0.9, 0.95, 0.99, 0.999])\n        opt_kwargs['beta_2'] = trial.suggest_categorical('opt_beta_2', [0.9, 0.95, 0.99, 0.999])\n    if opt_init == 'Nadam':\n        opt_kwargs['learning_rate'] = trial.suggest_float('opt_lr', 1e-2, 1e-1, log=True)\n        opt_kwargs['beta_1'] = trial.suggest_categorical('opt_beta_1', [0.9, 0.95, 0.99, 0.999])\n        opt_kwargs['beta_2'] = trial.suggest_categorical('opt_beta_2', [0.9, 0.95, 0.99, 0.999])\n    if opt_init == 'Adamax':\n        opt_kwargs['learning_rate'] = trial.suggest_float('opt_lr', 1e-2, 1e-1, log=True)\n        opt_kwargs['beta_1'] = trial.suggest_categorical('opt_beta_1', [0.9, 0.95, 0.99, 0.999])\n        opt_kwargs['beta_2'] = trial.suggest_categorical('opt_beta_2', [0.9, 0.95, 0.99, 0.999])\n    optimizer = getattr(tf.optimizers, opt_init)(**opt_kwargs)\n    return optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:05:24.491827Z","iopub.execute_input":"2025-01-02T22:05:24.492204Z","iopub.status.idle":"2025-01-02T22:05:24.520122Z","shell.execute_reply.started":"2025-01-02T22:05:24.492176Z","shell.execute_reply":"2025-01-02T22:05:24.518871Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Objective/train","metadata":{}},{"cell_type":"code","source":"INPUT_SHAPE = (14, 3, )\n\ndef objective(trial):\n    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256, 512, 1024])\n    train_ds_repeat, valid_ds_repeat, _, train_num_batches, valid_num_batches, _ = load_data(batch_size=batch_size)\n    \n    model = create_model(trial, input_shape=INPUT_SHAPE)\n    optimizer = create_optimizer(trial)\n    model.compile(\n        loss=tf.keras.losses.MeanAbsoluteError(),\n        optimizer=optimizer,\n        metrics=[tf.keras.metrics.MeanSquaredError()]\n    )\n    \n    # callbacks\n    logdir = os.path.join(\"logs/optuna\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n    earlystopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25)\n    lr_scheduler_callback = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss', patience=10,\n        factor=trial.suggest_float('lr_scheduler_factor', 0.1, 0.75)\n    )\n\n    history = model.fit(\n        train_ds_repeat, epochs=500, validation_data=valid_ds_repeat,\n        steps_per_epoch=train_num_batches, validation_steps=valid_num_batches,\n        callbacks=[tensorboard_callback, earlystopping_callback, lr_scheduler_callback],\n        verbose=0\n    )\n    print('\\n')\n    return np.min(history.history['val_loss'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:05:24.521279Z","iopub.execute_input":"2025-01-02T22:05:24.521667Z","iopub.status.idle":"2025-01-02T22:05:24.545725Z","shell.execute_reply.started":"2025-01-02T22:05:24.521629Z","shell.execute_reply":"2025-01-02T22:05:24.544422Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Run","metadata":{}},{"cell_type":"code","source":"sampler = optuna.samplers.TPESampler(\n    n_startup_trials=25, n_ei_candidates=24,\n    multivariate=False, seed=42\n)\nstudy = optuna.create_study(direction='minimize', sampler=sampler, study_name='study', storage='sqlite:///db.sqlite3')\nstudy.optimize(\n    objective, n_trials=1000,\n    timeout=3600*11, # in seconds\n    n_jobs=4,\n    show_progress_bar=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:05:24.546708Z","iopub.execute_input":"2025-01-02T22:05:24.547095Z","execution_failed":"2025-01-03T00:08:50.117Z"}},"outputs":[{"name":"stderr","text":"[I 2025-01-02 22:05:25,840] A new study created in RDB with name: study\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fb3c1493fe641eb92023e0babd672e6"}},"metadata":{}},{"name":"stdout","text":"\n\n[I 2025-01-02 22:12:21,721] Trial 2 finished with value: 3.0630533695220947 and parameters: {'batch_size': 1024, 'rnn_num_layers': 3, 'units_rnn': 4, 'actv_func_rnn_0': 'elu', 'bias_reg_init_rnn_layer0': 'l2', 'bias_reg_rnn_layer_0': 0.40051751951603914, 'kernel_reg_init_rnn_layer_0': 'l1l2', 'kernel_reg_rnn_layer_0_l1': 0.49147115285443993, 'kernel_reg_rnn_layer_0_l2': 0.0854380793526322, 'kernel_initializer_rnn_layer_0': 'lecun_normal', 'actv_func_rnn_1': 'leaky_relu', 'bias_reg_init_rnn_layer1': None, 'kernel_reg_init_rnn_layer_1': 'l2', 'kernel_reg_rnn_layer_1': 0.20156900771385441, 'kernel_initializer_rnn_layer_1': 'he_uniform', 'actv_func_rnn_2': 'elu', 'bias_reg_init_rnn_layer2': None, 'kernel_reg_init_rnn_layer_2': 'l2', 'kernel_reg_rnn_layer_2': 0.06721679227967016, 'kernel_initializer_rnn_layer_2': 'he_normal', 'nunints_layer_0': 32, 'actv_func_layer_0': 'sigmoid', 'bias_reg_init_layer_0': 'l2', 'bias_reg_layer_0': 0.37808155814276634, 'kernel_reg_init_layer_0': 'l2', 'kernel_reg_layer_0': 0.43782324264602523, 'kernel_initializer_layer_0': 'he_uniform', 'num_layers': 0, 'batch_norm': False, 'dropout_rate_layer_output': 0.8875426820407548, 'optimizer': 'Nadam', 'opt_lr': 0.03089051893957598, 'opt_beta_1': 0.999, 'opt_beta_2': 0.9, 'lr_scheduler_factor': 0.6037943817132615}. Best is trial 2 with value: 3.0630533695220947.\n\n\n[W 2025-01-02 22:19:34,372] Trial 4 failed with parameters: {'batch_size': 256, 'rnn_num_layers': 2, 'units_rnn': 16, 'actv_func_rnn_0': 'tanh', 'bias_reg_init_rnn_layer0': 'l1', 'bias_reg_rnn_layer_0': 0.4796210608448482, 'kernel_reg_init_rnn_layer_0': 'l1l2', 'kernel_reg_rnn_layer_0_l1': 0.43293029415752265, 'kernel_reg_rnn_layer_0_l2': 0.3884380999417322, 'kernel_initializer_rnn_layer_0': 'he_normal', 'actv_func_rnn_1': 'leaky_relu', 'bias_reg_init_rnn_layer1': 'l1', 'bias_reg_rnn_layer_1': 0.3217109640307053, 'kernel_reg_init_rnn_layer_1': 'l2', 'kernel_reg_rnn_layer_1': 0.06927739554967655, 'kernel_initializer_rnn_layer_1': 'glorot_uniform', 'nunints_layer_0': 160, 'actv_func_layer_0': 'sigmoid', 'bias_reg_init_layer_0': 'l1', 'bias_reg_layer_0': 0.421413403744359, 'kernel_reg_init_layer_0': 'l1l2', 'kernel_reg_layer_0_l1': 0.055852287818149625, 'kernel_reg_layer_0_l2': 0.30507305943388713, 'kernel_initializer_layer_0': 'he_uniform', 'num_layers': 0, 'batch_norm': True, 'dropout_rate_layer_output': 0.7001897024736393, 'optimizer': 'Nadam', 'opt_lr': 0.08617578238980732, 'opt_beta_1': 0.99, 'opt_beta_2': 0.95, 'lr_scheduler_factor': 0.11764419404190042} because of the following error: The value nan is not acceptable.\n[W 2025-01-02 22:19:34,381] Trial 4 failed with value nan.\n\n\n[I 2025-01-02 22:52:08,154] Trial 0 finished with value: 0.5611269474029541 and parameters: {'batch_size': 1024, 'rnn_num_layers': 4, 'units_rnn': 4, 'actv_func_rnn_0': 'leaky_relu', 'bias_reg_init_rnn_layer0': 'l1', 'bias_reg_rnn_layer_0': 0.05880879778326989, 'kernel_reg_init_rnn_layer_0': 'l2', 'kernel_reg_rnn_layer_0': 0.2835224315892676, 'kernel_initializer_rnn_layer_0': 'he_uniform', 'actv_func_rnn_1': 'gelu', 'bias_reg_init_rnn_layer1': 'l1l2', 'bias_reg_rnn_layer_1_l1': 0.35966147223371164, 'bias_reg_rnn_layer_1_l2': 0.03694853516949471, 'kernel_reg_init_rnn_layer_1': 'l2', 'kernel_reg_rnn_layer_1': 0.20772918697970796, 'kernel_initializer_rnn_layer_1': 'lecun_normal', 'actv_func_rnn_2': 'tanh', 'bias_reg_init_rnn_layer2': 'l2', 'bias_reg_rnn_layer_2': 0.39502149278423604, 'kernel_reg_init_rnn_layer_2': 'l1l2', 'kernel_reg_rnn_layer_2_l1': 0.4292674451918386, 'kernel_reg_rnn_layer_2_l2': 0.11330865951459017, 'kernel_initializer_rnn_layer_2': 'lecun_normal', 'actv_func_rnn_3': 'leaky_relu', 'bias_reg_init_rnn_layer3': 'l2', 'bias_reg_rnn_layer_3': 0.4144484553480901, 'kernel_reg_init_rnn_layer_3': 'l1', 'kernel_reg_rnn_layer_3': 0.23371381329687585, 'kernel_initializer_rnn_layer_3': 'glorot_normal', 'nunints_layer_0': 96, 'actv_func_layer_0': 'gelu', 'bias_reg_init_layer_0': None, 'kernel_reg_init_layer_0': 'l1', 'kernel_reg_layer_0': 0.16484688794002986, 'kernel_initializer_layer_0': 'he_uniform', 'num_layers': 2, 'batch_norm': False, 'nunits_layer_1': 224, 'actv_func_layer_1': 'relu', 'bias_reg_init_layer_1': 'l1l2', 'bias_reg_layer_1_l1': 0.05971658889094222, 'bias_reg_layer_1_l2': 0.15574686370660068, 'kernel_reg_init_layer_1': None, 'kernel_initializer_layer_1': 'lecun_uniform', 'dropout_rate_layer_1': 0.3515017889134222, 'nunits_layer_2': 96, 'actv_func_layer_2': 'relu', 'bias_reg_init_layer_2': 'l2', 'bias_reg_layer_2': 0.22622485432258438, 'kernel_reg_init_layer_2': None, 'kernel_initializer_layer_2': 'glorot_uniform', 'dropout_rate_layer_2': 0.11500904243287785, 'dropout_rate_layer_output': 0.14550990036233794, 'optimizer': 'SGD', 'opt_lr': 0.014236085162892861, 'opt_momentum': 0.01779924549193132, 'opt_nesterov': True, 'lr_scheduler_factor': 0.25323343708236556}. Best is trial 0 with value: 0.5611269474029541.\n\n\n[I 2025-01-02 23:24:17,144] Trial 5 finished with value: 0.8179786801338196 and parameters: {'batch_size': 1024, 'rnn_num_layers': 2, 'units_rnn': 16, 'actv_func_rnn_0': 'tanh', 'bias_reg_init_rnn_layer0': 'l1', 'bias_reg_rnn_layer_0': 0.2659135554711897, 'kernel_reg_init_rnn_layer_0': 'l1l2', 'kernel_reg_rnn_layer_0_l1': 0.1674118361110778, 'kernel_reg_rnn_layer_0_l2': 0.2706038577150466, 'kernel_initializer_rnn_layer_0': 'lecun_normal', 'actv_func_rnn_1': 'tanh', 'bias_reg_init_rnn_layer1': None, 'kernel_reg_init_rnn_layer_1': 'l2', 'kernel_reg_rnn_layer_1': 0.28659048294574746, 'kernel_initializer_rnn_layer_1': 'glorot_uniform', 'nunints_layer_0': 384, 'actv_func_layer_0': 'gelu', 'bias_reg_init_layer_0': 'l1l2', 'bias_reg_layer_0_l1': 0.13165948252710408, 'bias_reg_layer_0_l2': 0.00703907231283571, 'kernel_reg_init_layer_0': 'l2', 'kernel_reg_layer_0': 0.19225672359746, 'kernel_initializer_layer_0': 'he_uniform', 'num_layers': 1, 'batch_norm': False, 'nunits_layer_1': 160, 'actv_func_layer_1': 'elu', 'bias_reg_init_layer_1': None, 'kernel_reg_init_layer_1': 'l1l2', 'kernel_reg_layer_1_l1': 0.17213181857634213, 'kernel_reg_layer_1_l2': 0.09419288777645794, 'kernel_initializer_layer_1': 'glorot_normal', 'dropout_rate_layer_1': 0.7572236303532692, 'dropout_rate_layer_output': 0.08580136981412873, 'optimizer': 'Adam', 'opt_lr': 0.01909400581547424, 'opt_beta_1': 0.95, 'opt_beta_2': 0.99, 'lr_scheduler_factor': 0.36255567361467234}. Best is trial 0 with value: 0.5611269474029541.\n\n\n[I 2025-01-02 23:28:49,156] Trial 1 finished with value: 2.333960771560669 and parameters: {'batch_size': 512, 'rnn_num_layers': 1, 'units_rnn': 32, 'actv_func_rnn_0': 'sigmoid', 'bias_reg_init_rnn_layer0': None, 'kernel_reg_init_rnn_layer_0': 'l1l2', 'kernel_reg_rnn_layer_0_l1': 0.3571795319783835, 'kernel_reg_rnn_layer_0_l2': 0.47646289929575103, 'kernel_initializer_rnn_layer_0': 'lecun_normal', 'nunints_layer_0': 256, 'actv_func_layer_0': 'sigmoid', 'bias_reg_init_layer_0': None, 'kernel_reg_init_layer_0': 'l1l2', 'kernel_reg_layer_0_l1': 0.45820093525575367, 'kernel_reg_layer_0_l2': 0.16171617532953322, 'kernel_initializer_layer_0': 'he_uniform', 'num_layers': 1, 'batch_norm': True, 'nunits_layer_1': 192, 'actv_func_layer_1': 'sigmoid', 'bias_reg_init_layer_1': 'l1', 'bias_reg_layer_1': 0.4447965693087723, 'kernel_reg_init_layer_1': 'l2', 'kernel_reg_layer_1': 0.0856893977461946, 'kernel_initializer_layer_1': 'glorot_normal', 'dropout_rate_layer_1': 0.35162301696593784, 'dropout_rate_layer_output': 0.2402414056979079, 'optimizer': 'SGD', 'opt_lr': 0.014085856108045195, 'opt_momentum': 0.002692904431402747, 'opt_nesterov': False, 'lr_scheduler_factor': 0.5907997899693234}. Best is trial 0 with value: 0.5611269474029541.\n\n\n[I 2025-01-02 23:28:50,923] Trial 3 finished with value: 0.8213455080986023 and parameters: {'batch_size': 512, 'rnn_num_layers': 4, 'units_rnn': 4, 'actv_func_rnn_0': 'relu', 'bias_reg_init_rnn_layer0': 'l1', 'bias_reg_rnn_layer_0': 0.42939106989110776, 'kernel_reg_init_rnn_layer_0': 'l1', 'kernel_reg_rnn_layer_0': 0.2474364086084754, 'kernel_initializer_rnn_layer_0': 'glorot_uniform', 'actv_func_rnn_1': 'gelu', 'bias_reg_init_rnn_layer1': None, 'kernel_reg_init_rnn_layer_1': 'l1l2', 'kernel_reg_rnn_layer_1_l1': 0.04937301934433612, 'kernel_reg_rnn_layer_1_l2': 0.19257915491565356, 'kernel_initializer_rnn_layer_1': 'lecun_normal', 'actv_func_rnn_2': 'relu', 'bias_reg_init_rnn_layer2': 'l1l2', 'bias_reg_rnn_layer_2_l1': 0.48488960724210545, 'bias_reg_rnn_layer_2_l2': 0.22538904371390567, 'kernel_reg_init_rnn_layer_2': None, 'kernel_initializer_rnn_layer_2': 'glorot_uniform', 'actv_func_rnn_3': 'gelu', 'bias_reg_init_rnn_layer3': 'l1l2', 'bias_reg_rnn_layer_3_l1': 0.209653287306517, 'bias_reg_rnn_layer_3_l2': 0.37108105896863963, 'kernel_reg_init_rnn_layer_3': None, 'kernel_initializer_rnn_layer_3': 'he_normal', 'nunints_layer_0': 480, 'actv_func_layer_0': 'elu', 'bias_reg_init_layer_0': 'l1l2', 'bias_reg_layer_0_l1': 0.36009797245777997, 'bias_reg_layer_0_l2': 0.15902606698145855, 'kernel_reg_init_layer_0': 'l2', 'kernel_reg_layer_0': 0.4222881254695623, 'kernel_initializer_layer_0': 'glorot_uniform', 'num_layers': 2, 'batch_norm': False, 'nunits_layer_1': 256, 'actv_func_layer_1': 'sigmoid', 'bias_reg_init_layer_1': None, 'kernel_reg_init_layer_1': 'l1', 'kernel_reg_layer_1': 0.4317930148862815, 'kernel_initializer_layer_1': 'lecun_normal', 'dropout_rate_layer_1': 0.7027154377058531, 'nunits_layer_2': 480, 'actv_func_layer_2': 'tanh', 'bias_reg_init_layer_2': None, 'kernel_reg_init_layer_2': 'l1l2', 'kernel_reg_layer_2_l1': 0.3451987033059006, 'kernel_reg_layer_2_l2': 0.15463320636734443, 'kernel_initializer_layer_2': 'lecun_normal', 'dropout_rate_layer_2': 0.46953078453499153, 'dropout_rate_layer_output': 0.5856541781540026, 'optimizer': 'Adam', 'opt_lr': 0.06579610632640137, 'opt_beta_1': 0.95, 'opt_beta_2': 0.9, 'lr_scheduler_factor': 0.13980530260637858}. Best is trial 0 with value: 0.5611269474029541.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}