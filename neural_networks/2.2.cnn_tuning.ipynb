{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.metrics import mean_absolute_error\nimport tensorflow as tf\nimport optuna\nimport os\nimport random\nimport datetime\n\nnp.random.seed(42)\ntf.random.set_seed(42)\nrandom.seed(42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:19:48.340726Z","iopub.execute_input":"2024-12-31T11:19:48.341147Z","iopub.status.idle":"2024-12-31T11:19:58.723376Z","shell.execute_reply.started":"2024-12-31T11:19:48.341112Z","shell.execute_reply":"2024-12-31T11:19:58.722387Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"tf.config.list_physical_devices()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:19:58.724767Z","iopub.execute_input":"2024-12-31T11:19:58.725412Z","iopub.status.idle":"2024-12-31T11:19:58.734358Z","shell.execute_reply.started":"2024-12-31T11:19:58.725369Z","shell.execute_reply":"2024-12-31T11:19:58.733206Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"data_path = 'https://raw.githubusercontent.com/antbartash/max_temp/master/data/data2.csv'\ndata = pd.read_csv(data_path, index_col=0)\ndata['DATE'] = data['DATE'].astype('datetime64[ns]')\n\nprint(data.shape)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:19:58.736071Z","iopub.execute_input":"2024-12-31T11:19:58.736432Z","iopub.status.idle":"2024-12-31T11:19:59.101074Z","shell.execute_reply.started":"2024-12-31T11:19:58.736402Z","shell.execute_reply":"2024-12-31T11:19:59.100161Z"}},"outputs":[{"name":"stdout","text":"(40898, 4)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"       STATION                        NAME       DATE  TMAX\n0  USW00012916  NEW ORLEANS AIRPORT, LA US 2010-01-01  12.2\n1  USW00012916  NEW ORLEANS AIRPORT, LA US 2010-01-02  10.6\n2  USW00012916  NEW ORLEANS AIRPORT, LA US 2010-01-03   8.3\n3  USW00012916  NEW ORLEANS AIRPORT, LA US 2010-01-04   6.1\n4  USW00012916  NEW ORLEANS AIRPORT, LA US 2010-01-05   6.1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>STATION</th>\n      <th>NAME</th>\n      <th>DATE</th>\n      <th>TMAX</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>USW00012916</td>\n      <td>NEW ORLEANS AIRPORT, LA US</td>\n      <td>2010-01-01</td>\n      <td>12.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>USW00012916</td>\n      <td>NEW ORLEANS AIRPORT, LA US</td>\n      <td>2010-01-02</td>\n      <td>10.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>USW00012916</td>\n      <td>NEW ORLEANS AIRPORT, LA US</td>\n      <td>2010-01-03</td>\n      <td>8.3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>USW00012916</td>\n      <td>NEW ORLEANS AIRPORT, LA US</td>\n      <td>2010-01-04</td>\n      <td>6.1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>USW00012916</td>\n      <td>NEW ORLEANS AIRPORT, LA US</td>\n      <td>2010-01-05</td>\n      <td>6.1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"data = data[['STATION', 'DATE', 'TMAX']]\n\n# add sine and cosine transforms to add periodicality\ndoy = data['DATE'].dt.dayofyear / 365.25\ndata['Year_sin'] = np.sin(doy * 2 * np.pi)\ndata['Year_cos'] = np.cos(doy * 2 * np.pi)\n\nprint(data.shape)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:19:59.102325Z","iopub.execute_input":"2024-12-31T11:19:59.102752Z","iopub.status.idle":"2024-12-31T11:19:59.127069Z","shell.execute_reply.started":"2024-12-31T11:19:59.102722Z","shell.execute_reply":"2024-12-31T11:19:59.125698Z"}},"outputs":[{"name":"stdout","text":"(40898, 5)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"       STATION       DATE  TMAX  Year_sin  Year_cos\n0  USW00012916 2010-01-01  12.2  0.017202  0.999852\n1  USW00012916 2010-01-02  10.6  0.034398  0.999408\n2  USW00012916 2010-01-03   8.3  0.051584  0.998669\n3  USW00012916 2010-01-04   6.1  0.068755  0.997634\n4  USW00012916 2010-01-05   6.1  0.085906  0.996303","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>STATION</th>\n      <th>DATE</th>\n      <th>TMAX</th>\n      <th>Year_sin</th>\n      <th>Year_cos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>USW00012916</td>\n      <td>2010-01-01</td>\n      <td>12.2</td>\n      <td>0.017202</td>\n      <td>0.999852</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>USW00012916</td>\n      <td>2010-01-02</td>\n      <td>10.6</td>\n      <td>0.034398</td>\n      <td>0.999408</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>USW00012916</td>\n      <td>2010-01-03</td>\n      <td>8.3</td>\n      <td>0.051584</td>\n      <td>0.998669</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>USW00012916</td>\n      <td>2010-01-04</td>\n      <td>6.1</td>\n      <td>0.068755</td>\n      <td>0.997634</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>USW00012916</td>\n      <td>2010-01-05</td>\n      <td>6.1</td>\n      <td>0.085906</td>\n      <td>0.996303</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{"execution":{"iopub.status.busy":"2024-12-28T11:24:06.993325Z","iopub.execute_input":"2024-12-28T11:24:06.993684Z","iopub.status.idle":"2024-12-28T11:24:06.997628Z","shell.execute_reply.started":"2024-12-28T11:24:06.993646Z","shell.execute_reply":"2024-12-28T11:24:06.996626Z"}}},{"cell_type":"code","source":"# TRAIN/VALID/TEST SPLIT\n\ntrain_df = data.loc[data['DATE'].dt.year <= 2021].reset_index(drop=True).copy()\nvalid_df = data.loc[data['DATE'].dt.year == 2022].reset_index(drop=True).copy()\ntest_df = data.loc[data['DATE'].dt.year == 2023].reset_index(drop=True).copy()\n\nprint(f'Train: {train_df.shape}')\nprint(f'Valid: {valid_df.shape}')\nprint(f'Test: {test_df.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:19:59.128030Z","iopub.execute_input":"2024-12-31T11:19:59.128357Z","iopub.status.idle":"2024-12-31T11:19:59.150989Z","shell.execute_reply.started":"2024-12-31T11:19:59.128331Z","shell.execute_reply":"2024-12-31T11:19:59.149765Z"}},"outputs":[{"name":"stdout","text":"Train: (35058, 5)\nValid: (2920, 5)\nTest: (2920, 5)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# SCALING\n\ntrain_df.drop(columns=['DATE'], inplace=True)\nvalid_df.drop(columns=['DATE'], inplace=True)\ntest_df.drop(columns=['DATE'], inplace=True)\n\n# keep station to drop mixed windows later (encode to avoid errors from scaler)\nencoder = OrdinalEncoder()\nencoder.fit(train_df[['STATION']])\ntrain_df['STATION'] = encoder.transform(train_df[['STATION']])[:, 0]\nvalid_df['STATION'] = encoder.transform(valid_df[['STATION']])[:, 0]\ntest_df['STATION'] = encoder.transform(test_df[['STATION']])[:, 0]\n\n# scaling\nscaler = StandardScaler()\nscaler.fit(train_df)\ntrain_df = pd.DataFrame(scaler.transform(train_df),\n                        columns=scaler.feature_names_in_, index=train_df.index)\nvalid_df = pd.DataFrame(scaler.transform(valid_df),\n                        columns=scaler.feature_names_in_, index=valid_df.index)\ntest_df = pd.DataFrame(scaler.transform(test_df),\n                       columns=scaler.feature_names_in_, index=test_df.index)\n\ntrain_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:19:59.152156Z","iopub.execute_input":"2024-12-31T11:19:59.152483Z","iopub.status.idle":"2024-12-31T11:19:59.194271Z","shell.execute_reply.started":"2024-12-31T11:19:59.152456Z","shell.execute_reply":"2024-12-31T11:19:59.193017Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"    STATION      TMAX  Year_sin  Year_cos\n0 -1.091014 -0.747812  0.024401  1.414059\n1 -1.091014 -0.896450  0.048721  1.413431\n2 -1.091014 -1.110117  0.073027  1.412386\n3 -1.091014 -1.314495  0.097312  1.410922\n4 -1.091014 -1.314495  0.121567  1.409041","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>STATION</th>\n      <th>TMAX</th>\n      <th>Year_sin</th>\n      <th>Year_cos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.091014</td>\n      <td>-0.747812</td>\n      <td>0.024401</td>\n      <td>1.414059</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.091014</td>\n      <td>-0.896450</td>\n      <td>0.048721</td>\n      <td>1.413431</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.091014</td>\n      <td>-1.110117</td>\n      <td>0.073027</td>\n      <td>1.412386</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.091014</td>\n      <td>-1.314495</td>\n      <td>0.097312</td>\n      <td>1.410922</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.091014</td>\n      <td>-1.314495</td>\n      <td>0.121567</td>\n      <td>1.409041</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def tmax_inverse_transform(arr, scale=scaler.scale_[1], mean=scaler.mean_[1]):\n    return arr * scale + mean","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:19:59.195273Z","iopub.execute_input":"2024-12-31T11:19:59.195619Z","iopub.status.idle":"2024-12-31T11:19:59.200102Z","shell.execute_reply.started":"2024-12-31T11:19:59.195547Z","shell.execute_reply":"2024-12-31T11:19:59.198808Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# TF DATASET\n\ndef create_dataset(data, target_col, source_col, seq_length):\n  input_data = data[:-seq_length]\n  # adding source_col to target allows to drop samples with features and target\n  # from diff sources (source_col will be dropped from the target later)\n  targets = data[[source_col, target_col]][seq_length:]\n  dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n      input_data, targets,\n      sequence_length=seq_length,\n      sequence_stride=1,\n      batch_size=32,\n      shuffle=False,\n      seed=42\n    )\n  for batch in dataset:\n    inputs, targets = batch\n    assert np.array_equal(inputs[0], data[:seq_length])  # First sequence: steps [0-13]\n    # Corresponding target: step 14\n    assert np.array_equal(targets[0, 1], data.loc[seq_length, target_col])\n    break\n  return dataset\n\ntrain_ds = create_dataset(train_df, 'TMAX', 'STATION', 14)\nvalid_ds = create_dataset(valid_df, 'TMAX', 'STATION', 14)\ntest_ds = create_dataset(test_df, 'TMAX', 'STATION', 14)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:19:59.202502Z","iopub.execute_input":"2024-12-31T11:19:59.202799Z","iopub.status.idle":"2024-12-31T11:19:59.719691Z","shell.execute_reply.started":"2024-12-31T11:19:59.202777Z","shell.execute_reply":"2024-12-31T11:19:59.718761Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# DATASET CLEANING\n\ndef filter_mixed_windows(dataset):\n    def is_valid_window(inputs, targets):\n        # Extract the first feature (station) from inputs and targets\n        input_station_ids = inputs[:, :, 0]  # Shape: (batch_size, sequence_length)\n        target_station_ids = targets[:, 0]  # Shape: (batch_size)\n\n        # Check if all station IDs in the inputs are the same\n        input_same_station = tf.reduce_all(tf.reduce_max(input_station_ids, axis=1) == tf.reduce_min(input_station_ids, axis=1))\n\n        # Check if the target's station ID matches the input station ID\n        target_matches_input = tf.reduce_all(tf.reduce_max(input_station_ids, axis=1) == target_station_ids)\n\n        # Only keep windows where both conditions are true\n        return tf.logical_and(input_same_station, target_matches_input)\n\n    # Filter the dataset\n    filtered_dataset = dataset.filter(is_valid_window)\n    return filtered_dataset\n\ntrain_ds = filter_mixed_windows(train_ds)\nvalid_ds = filter_mixed_windows(valid_ds)\ntest_ds = filter_mixed_windows(test_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:19:59.721449Z","iopub.execute_input":"2024-12-31T11:19:59.722014Z","iopub.status.idle":"2024-12-31T11:20:00.048533Z","shell.execute_reply.started":"2024-12-31T11:19:59.721970Z","shell.execute_reply":"2024-12-31T11:20:00.047325Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def drop_first_column(feature, label):\n    feature = feature[:, :, 1:]  # Keep all rows, drop the first column\n    label = label[:, 1:]    # Keep all rows, drop the first column\n    return feature, label\n\ntrain_ds = train_ds.map(drop_first_column)\nvalid_ds = valid_ds.map(drop_first_column)\ntest_ds = test_ds.map(drop_first_column)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:20:00.049979Z","iopub.execute_input":"2024-12-31T11:20:00.050312Z","iopub.status.idle":"2024-12-31T11:20:00.130870Z","shell.execute_reply.started":"2024-12-31T11:20:00.050282Z","shell.execute_reply":"2024-12-31T11:20:00.129678Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def load_data(batch_size, prefetch=tf.data.AUTOTUNE,\n              train_ds=train_ds, valid_ds=valid_ds, test_ds=test_ds):\n    train_ds = train_ds.rebatch(batch_size).prefetch(prefetch)\n    valid_ds = valid_ds.rebatch(batch_size).prefetch(prefetch)\n    test_ds = test_ds.rebatch(batch_size).prefetch(prefetch)\n    train_num_batches = len(list(train_ds))\n    valid_num_batches = len(list(valid_ds))\n    test_num_batches = len(list(test_ds))\n    # print(f'load_data - num_batches (train, valid, test): {train_num_batches}, {valid_num_batches}, {test_num_batches}')\n    train_ds_repeat = train_ds.repeat()\n    valid_ds_repeat = valid_ds.repeat()\n    test_ds_repeat = test_ds.repeat()\n    return train_ds_repeat, valid_ds_repeat, test_ds_repeat, train_num_batches, valid_num_batches, test_num_batches\n\ntrain_ds_repeat, valid_ds_repeat, test_ds_repeat, train_num_batches, valid_num_batches, test_num_batches = load_data(batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:20:00.132313Z","iopub.execute_input":"2024-12-31T11:20:00.132766Z","iopub.status.idle":"2024-12-31T11:20:03.614973Z","shell.execute_reply.started":"2024-12-31T11:20:00.132721Z","shell.execute_reply":"2024-12-31T11:20:03.613890Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def create_model(trial, input_shape):\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Input(shape=input_shape))\n    model.add(tf.keras.layers.Reshape(target_shape=(14, 3, 1)))  # Shape: (time, features) => (time, features, channels)\n    \n    # CONVOLUTIONAL LAYERS\n    conv_num_layers = trial.suggest_int('conv_num_layers', 1, 4)\n    for conv_layer_num in range(conv_num_layers):\n        filters = trial.suggest_categorical(f'filters_conv_{conv_layer_num}', [4, 8, 16, 32, 64])\n        kernel_size = trial.suggest_categorical(f'kernel_size_conv_{conv_layer_num}', [3, 5, 7, 10, 14])\n        actv_func = trial.suggest_categorical(f'actv_func_conv_{conv_layer_num}', ['relu', 'leaky_relu', 'elu', 'sigmoid', 'tanh', 'gelu'])\n        \n        bias_reg_init = trial.suggest_categorical(f'bias_reg_init_conv_layer{conv_layer_num}', ['l1', 'l2', 'l1l2', None])\n        if bias_reg_init == 'l1':\n            bias_reg = tf.keras.regularizers.l1(trial.suggest_float(f'bias_reg_conv_layer_{conv_layer_num}', 0.0001, 0.5))\n        elif bias_reg_init == 'l2':\n            bias_reg = tf.keras.regularizers.l2(trial.suggest_float(f'bias_reg_conv_layer_{conv_layer_num}', 0.0001, 0.5))\n        elif bias_reg_init == 'l1l2':\n            bias_reg = tf.keras.regularizers.L1L2(trial.suggest_float(f'bias_reg_conv_layer_{conv_layer_num}_l1', 0.0001, 0.5),\n                                                  trial.suggest_float(f'bias_reg_conv_layer_{conv_layer_num}_l2', 0.0001, 0.5))\n        else:\n            bias_reg = None\n\n        kernel_reg_init = trial.suggest_categorical(f'kernel_reg_init_conv_layer_{conv_layer_num}', ['l1', 'l2', 'l1l2', None])\n        if kernel_reg_init == 'l1':\n            kernel_reg = tf.keras.regularizers.l1(trial.suggest_float(f'kernel_reg_conv_layer_{conv_layer_num}', 0.0001, 0.5))\n        elif kernel_reg_init == 'l2':\n            kernel_reg = tf.keras.regularizers.l2(trial.suggest_float(f'kernel_reg_conv_layer_{conv_layer_num}', 0.0001, 0.5))\n        elif kernel_reg_init == 'l1l2':\n            kernel_reg = tf.keras.regularizers.L1L2(trial.suggest_float(f'kernel_reg_conv_layer_{conv_layer_num}_l1', 0.0001, 0.5),\n                                                    trial.suggest_float(f'kernel_reg_conv_layer_{conv_layer_num}_l2', 0.0001, 0.5))\n        else:\n            kernel_reg = None\n\n        kernel_initializer = trial.suggest_categorical(\n            f'kernel_initializer_conv_layer_{conv_layer_num}',\n            ['glorot_uniform', 'glorot_normal', 'he_uniform', 'he_normal', 'lecun_uniform', 'lecun_normal']\n        )\n\n        if conv_layer_num == conv_num_layers-1: # last conv layer\n            padding = trial.suggest_categorical('padding_last_conv_layer', ['same', 'valid'])\n        else:\n            padding = 'same'\n\n        model.add(\n            tf.keras.layers.Conv2D(\n                filters=filters,\n                kernel_size=(kernel_size, 3),\n                strides=(1, 1),\n                padding=padding,\n                activation=actv_func,\n                kernel_initializer=kernel_initializer,\n                bias_regularizer=bias_reg,\n                kernel_regularizer=kernel_reg\n            )\n        )\n\n    \n    # FLATTEN\n    model.add(tf.keras.layers.Flatten())\n    \n    # DENSE LAYERS\n    # layer 0\n    n_units_0 = trial.suggest_int('nunints_layer_0', 32, 512, step=32)\n    actv_func_0 = trial.suggest_categorical('actv_func_layer_0', ['relu', 'leaky_relu', 'elu', 'sigmoid', 'tanh', 'gelu'])\n\n    bias_reg_init_0 = trial.suggest_categorical('bias_reg_init_layer_0', ['l1', 'l2', 'l1l2', None])\n    if bias_reg_init_0 == 'l1':\n        bias_reg_0 = tf.keras.regularizers.l1(trial.suggest_float('bias_reg_layer_0', 0.0001, 0.5))\n    elif bias_reg_init_0 == 'l2':\n        bias_reg_0 = tf.keras.regularizers.l2(trial.suggest_float('bias_reg_layer_0', 0.0001, 0.5))\n    elif bias_reg_init_0 == 'l1l2':\n        bias_reg_0 = tf.keras.regularizers.L1L2(trial.suggest_float('bias_reg_layer_0_l1', 0.0001, 0.5),\n                                                trial.suggest_float('bias_reg_layer_0_l2', 0.0001, 0.5))\n    else:\n        bias_reg_0 = None\n\n    kernel_reg_init_0 = trial.suggest_categorical('kernel_reg_init_layer_0', ['l1', 'l2', 'l1l2', None])\n    if kernel_reg_init_0 == 'l1':\n        kernel_reg_0 = tf.keras.regularizers.l1(trial.suggest_float('kernel_reg_layer_0', 0.0001, 0.5))\n    elif kernel_reg_init_0 == 'l2':\n        kernel_reg_0 = tf.keras.regularizers.l2(trial.suggest_float('kernel_reg_layer_0', 0.0001, 0.5))\n    elif kernel_reg_init_0 == 'l1l2':\n        kernel_reg_0 = tf.keras.regularizers.L1L2(trial.suggest_float('kernel_reg_layer_0_l1', 0.0001, 0.5),\n                                                  trial.suggest_float('kernel_reg_layer_0_l2', 0.0001, 0.5))\n    else:\n        kernel_reg_0 = None\n\n    kernel_initializer_0 = trial.suggest_categorical(\n        'kernel_initializer_layer_0', ['glorot_uniform', 'glorot_normal',\n                                       'he_uniform', 'he_normal',\n                                       'lecun_uniform', 'lecun_normal']\n    )\n    \n    model.add(\n        tf.keras.layers.Dense(\n            units=n_units_0, activation=actv_func_0, \n            kernel_initializer=kernel_initializer_0,\n            bias_regularizer=bias_reg_0,\n            kernel_regularizer=kernel_reg_0\n        )\n    )\n\n    \n    # hidden layers\n    num_layers = trial.suggest_int('num_layers', 0, 2)\n    batch_norm = trial.suggest_categorical(f'batch_norm', [True, False])\n    for layer_num in range(num_layers):\n        layer_i = layer_num + 1\n        n_units = trial.suggest_int(f'nunits_layer_{layer_i}', 32, 512, step=32)\n        actv_func = trial.suggest_categorical(f'actv_func_layer_{layer_i}', ['relu', 'leaky_relu', 'elu', 'sigmoid', 'tanh', 'gelu'])\n\n        bias_reg_init = trial.suggest_categorical(f'bias_reg_init_layer_{layer_i}', ['l1', 'l2', 'l1l2', None])\n        if bias_reg_init == 'l1':\n            bias_reg = tf.keras.regularizers.l1(trial.suggest_float(f'bias_reg_layer_{layer_i}', 0.0001, 0.5))\n        elif bias_reg_init == 'l2':\n            bias_reg = tf.keras.regularizers.l2(trial.suggest_float(f'bias_reg_layer_{layer_i}', 0.0001, 0.5))\n        elif bias_reg_init == 'l1l2':\n            bias_reg = tf.keras.regularizers.L1L2(trial.suggest_float(f'bias_reg_layer_{layer_i}_l1', 0.0001, 0.5),\n                                                  trial.suggest_float(f'bias_reg_layer_{layer_i}_l2', 0.0001, 0.5))\n        else:\n            bias_reg = None\n\n        kernel_reg_init = trial.suggest_categorical(f'kernel_reg_init_layer_{layer_i}', ['l1', 'l2', 'l1l2', None])\n        if kernel_reg_init == 'l1':\n            kernel_reg = tf.keras.regularizers.l1(trial.suggest_float(f'kernel_reg_layer_{layer_i}', 0.0001, 0.5))\n        elif kernel_reg_init == 'l2':\n            kernel_reg = tf.keras.regularizers.l2(trial.suggest_float(f'kernel_reg_layer_{layer_i}', 0.0001, 0.5))\n        elif kernel_reg_init == 'l1l2':\n            kernel_reg = tf.keras.regularizers.L1L2(trial.suggest_float(f'kernel_reg_layer_{layer_i}_l1', 0.0001, 0.5),\n                                                    trial.suggest_float(f'kernel_reg_layer_{layer_i}_l2', 0.0001, 0.5))\n        else:\n            kernel_reg = None\n\n        kernel_initializer = trial.suggest_categorical(\n            f'kernel_initializer_layer_{layer_i}', ['glorot_uniform', 'glorot_normal',\n                                                    'he_uniform', 'he_normal',\n                                                    'lecun_uniform', 'lecun_normal']\n        )\n        \n        if batch_norm:\n            model.add(tf.keras.layers.BatchNormalization())\n        dropout_rate = trial.suggest_float(f'dropout_rate_layer_{layer_i}', 0.0, 0.999)\n        model.add(tf.keras.layers.Dropout(dropout_rate))\n        model.add(\n            tf.keras.layers.Dense(\n                n_units, actv_func, \n                kernel_initializer=kernel_initializer,\n                bias_regularizer=bias_reg,\n                kernel_regularizer=kernel_reg\n            )\n        )\n    if batch_norm:\n        model.add(tf.keras.layers.BatchNormalization())\n    dropout_rate = trial.suggest_float(f'dropout_rate_layer_output', 0.0, 0.999)\n    model.add(tf.keras.layers.Dropout(dropout_rate))\n    model.add(tf.keras.layers.Dense(1, activation='linear'))\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:20:03.616089Z","iopub.execute_input":"2024-12-31T11:20:03.616391Z","iopub.status.idle":"2024-12-31T11:20:03.638684Z","shell.execute_reply.started":"2024-12-31T11:20:03.616365Z","shell.execute_reply":"2024-12-31T11:20:03.637501Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Optimizer","metadata":{}},{"cell_type":"code","source":"def create_optimizer(trial):\n    opt_kwargs = {}\n    opt_init = trial.suggest_categorical('optimizer', ['SGD', 'Adam', 'Nadam', 'Adamax'])\n    if opt_init == 'SGD':\n        opt_kwargs['learning_rate'] = trial.suggest_float('opt_lr', 1e-2, 1e-1, log=True)\n        opt_kwargs['momentum'] = trial.suggest_float('opt_momentum', 1e-5, 0.1, log=True)\n        opt_kwargs['nesterov'] = trial.suggest_categorical('opt_nesterov', [True, False])\n    if opt_init == 'Adam':\n        opt_kwargs['learning_rate'] = trial.suggest_float('opt_lr', 1e-2, 1e-1, log=True)\n        opt_kwargs['beta_1'] = trial.suggest_categorical('opt_beta_1', [0.9, 0.95, 0.99, 0.999])\n        opt_kwargs['beta_2'] = trial.suggest_categorical('opt_beta_2', [0.9, 0.95, 0.99, 0.999])\n    if opt_init == 'Nadam':\n        opt_kwargs['learning_rate'] = trial.suggest_float('opt_lr', 1e-2, 1e-1, log=True)\n        opt_kwargs['beta_1'] = trial.suggest_categorical('opt_beta_1', [0.9, 0.95, 0.99, 0.999])\n        opt_kwargs['beta_2'] = trial.suggest_categorical('opt_beta_2', [0.9, 0.95, 0.99, 0.999])\n    if opt_init == 'Adamax':\n        opt_kwargs['learning_rate'] = trial.suggest_float('opt_lr', 1e-2, 1e-1, log=True)\n        opt_kwargs['beta_1'] = trial.suggest_categorical('opt_beta_1', [0.9, 0.95, 0.99, 0.999])\n        opt_kwargs['beta_2'] = trial.suggest_categorical('opt_beta_2', [0.9, 0.95, 0.99, 0.999])\n    optimizer = getattr(tf.optimizers, opt_init)(**opt_kwargs)\n    return optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:20:03.640057Z","iopub.execute_input":"2024-12-31T11:20:03.640468Z","iopub.status.idle":"2024-12-31T11:20:03.662728Z","shell.execute_reply.started":"2024-12-31T11:20:03.640430Z","shell.execute_reply":"2024-12-31T11:20:03.661548Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Objective/train","metadata":{}},{"cell_type":"code","source":"INPUT_SHAPE = (14, 3, )\n\ndef objective(trial):\n    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256, 512, 1024])\n    train_ds_repeat, valid_ds_repeat, _, train_num_batches, valid_num_batches, _ = load_data(batch_size=batch_size)\n    \n    model = create_model(trial, input_shape=INPUT_SHAPE)\n    optimizer = create_optimizer(trial)\n    model.compile(\n        loss=tf.keras.losses.MeanAbsoluteError(),\n        optimizer=optimizer,\n        metrics=[tf.keras.metrics.MeanSquaredError()]\n    )\n    \n    # callbacks\n    logdir = os.path.join(\"logs/optuna\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n    earlystopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25)\n    lr_scheduler_callback = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss', patience=10,\n        factor=trial.suggest_float('lr_scheduler_factor', 0.1, 0.75)\n    )\n\n    history = model.fit(\n        train_ds_repeat, epochs=500, validation_data=valid_ds_repeat,\n        steps_per_epoch=train_num_batches, validation_steps=valid_num_batches,\n        callbacks=[tensorboard_callback, earlystopping_callback, lr_scheduler_callback],\n        verbose=0\n    )\n    print('\\n')\n    return np.min(history.history['val_loss'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:20:03.664159Z","iopub.execute_input":"2024-12-31T11:20:03.664558Z","iopub.status.idle":"2024-12-31T11:20:03.687982Z","shell.execute_reply.started":"2024-12-31T11:20:03.664517Z","shell.execute_reply":"2024-12-31T11:20:03.686838Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Run","metadata":{}},{"cell_type":"code","source":"sampler = optuna.samplers.TPESampler(\n    n_startup_trials=25, n_ei_candidates=24,\n    multivariate=False, seed=42\n)\nstudy = optuna.create_study(direction='minimize', sampler=sampler, study_name='study', storage='sqlite:///db.sqlite3')\nstudy.optimize(\n    objective, n_trials=1000,\n    timeout=3600*11, # in seconds\n    n_jobs=4,\n    show_progress_bar=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:20:03.689223Z","iopub.execute_input":"2024-12-31T11:20:03.689538Z"}},"outputs":[{"name":"stderr","text":"[I 2024-12-31 11:20:04,878] A new study created in RDB with name: study\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4c826d5f24b4119947f1746e46005a8"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}